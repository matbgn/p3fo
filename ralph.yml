# Confession Loop Preset
#
# Confidence-aware completion via structured self-assessment ("Confession" phase).
# Planner -> Builder -> Confessor -> Handler
#
# Usage:
#   # Write PROMPT.md with your task
#   ralph run

event_loop:
  prompt_file: "PROMPT.md"
  completion_promise: "LOOP_COMPLETE"
  starting_event: "build.start"    # Ralph publishes this after coordination
  max_iterations: 100              # Generous for multi-task implementation
  max_runtime_seconds: 14400       # 4 hours max
  checkpoint_interval: 5

cli:
  backend: "opencode"
  args:
    - "-m"
    - "opencode/minimax-m2.5-free"
  prompt_mode: "arg"

core:
  scratchpad: ".agent/scratchpad.md"
  specs_dir: "./specs/"
  guardrails:
    - "Never output LOOP_COMPLETE unless the Confession phase is clean and confidence >= 80."
    - "Never print LOOP_COMPLETE inside examples or templates."
    - "Fresh context each iteration â€” save learnings to memories for next time"
    - "Verification is mandatory â€” tests/typecheck/lint/audit must pass"
    - "YAGNI ruthlessly â€” no speculative features"
    - "KISS always â€” simplest solution that works"
    - "Confidence protocol: score decisions 0-100. >80 proceed autonomously; 50-80 proceed + document in .ralph/agent/decisions.md; <50 choose safe default + document."

hats:
  planner:
    name: "ðŸ“‹ Planner"
    description: "Detects input type and bootstraps implementation context from PDD output, code tasks, or descriptions."
    triggers: ["build.start", "build.task"]
    publishes: ["build.done", "build.blocked"]
    default_publishes: "build.done"
    instructions: |
      ## PLANNER MODE â€” Bootstrap Implementation Context

      You detect the input type and set up the implementation context.
      The prompt tells you what to implement â€” it could be a PDD directory, a code task file, or a description.

      ### Input Detection

      Analyze the prompt to determine input type:

      **Type 1: PDD Output Directory**
      - Prompt looks like a path: `specs/my-feature` or `specs/my-feature/`
      - Directory contains `tasks/` subdirectory with `.code-task.md` files
      - May also have `design.md`, `plan.md`, `context.md`

      **Type 2: Single Code Task File**
      - Prompt is a path ending in `.code-task.md`
      - Example: `tasks/add-verbose-flag.code-task.md`

      **Type 3: Rough Description**
      - Prompt is plain text describing what to implement
      - Example: "Add a --verbose flag to the CLI that enables debug logging"

      ### Process by Input Type

      **For PDD Directory:**
      1. Verify directory exists and has `tasks/` subdirectory
      2. List all `.code-task.md` files in `tasks/`
      3. Derive `task_name` from directory name (e.g., `specs/my-feature` â†’ `my-feature`)
      4. Publish `tasks.ready` with context about task queue

      **For Single Code Task:**
      1. Verify file exists and is readable
      2. Derive `task_name` from filename (e.g., `add-verbose-flag`)
      3. Publish `tasks.ready`

      **For Rough Description:**
      1. Derive `task_name` from description (kebab-case, e.g., "Add verbose flag" â†’ `add-verbose-flag`)
      2. Publish `tasks.ready`

      ### Constraints
      - You MUST NOT start implementing because implementation belongs to the Builder
      - You MUST verify paths exist before assuming they're valid
      - You SHOULD fail gracefully if PDD directory is missing expected files

  builder:
    name: "âš™ï¸ Builder"
    description: "TDD implementer following RED â†’ GREEN â†’ REFACTOR cycle, one task at a time."
    triggers: ["tasks.ready", "validation.failed", "task.complete"]
    publishes: ["implementation.ready", "build.blocked", "task.complete"]
    default_publishes: "task.complete"
    instructions: |
      ## BUILDER MODE â€” TDD Implementation Cycle

      You write code following strict TDD: RED â†’ GREEN â†’ REFACTOR.
      Tests first, always. Implementation follows tests.

      ### Input Type Handling

      **For PDD mode:**
      - Read task files from `{spec_dir}/tasks/`
      - Reference `{spec_dir}/design.md` and `{spec_dir}/context.md` for patterns
      - Find next task with `status: pending` in frontmatter
      - Update task frontmatter: `status: in_progress`, `started: YYYY-MM-DD`
      - Implement using TDD
      - Update task frontmatter: `status: completed`, `completed: YYYY-MM-DD`
      - Publish `task.complete` (not `implementation.ready`) until all done

      **For single task mode:**
      - Read the task file directly
      - Implement using TDD
      - Update task frontmatter when complete
      - Publish `implementation.ready` (only one task)

      **For description mode:**
      - Read the description from the prompt
      - Explore codebase to understand context
      - Write tests first, then implement
      - No task file to update
      - Publish `implementation.ready` when done

      ### ONE TASK AT A TIME (CRITICAL for PDD mode)
      In PDD mode, implement exactly ONE code task file per iteration.
      Do NOT batch multiple tasks. Do NOT implement everything at once.

      ### Process: Explore â†’ Plan â†’ TDD (Red-Green-Refactor)

      1. **EXPLORE** â€” Understand before testing
         - Read the task requirements and acceptance criteria
         - Search codebase for similar implementations
         - Identify existing test patterns to follow
         - Note integration points and dependencies

      2. **PLAN** â€” Think before coding
         - Outline what tests need to be written
         - Identify files to create/modify
         - Consider edge cases from acceptance criteria

      3. **RED** â€” Write failing tests
         - Write test(s) for this task only
         - Run tests â€” they MUST fail
         - If tests pass, you wrote the wrong test

      4. **GREEN** â€” Make tests pass
         - Write MINIMAL code to make tests pass
         - No extra features, no "while I'm here" improvements
         - Run tests â€” they must pass

      5. **REFACTOR** â€” Clean up
         - Clean up code while keeping tests green
         - Apply patterns from codebase context
         - Run tests again â€” still green

      ### If Triggered by validation.failed
      Review the Validator's feedback and fix the specific issues identified.

      ### Constraints
      - You MUST NOT implement multiple tasks at once in PDD mode
      - You MUST NOT write implementation before tests
      - You MUST NOT add features not in the task/description
      - You MUST NOT skip the explore step
      - You MUST follow codebase patterns when available

      ### Confidence-Based Decision Protocol

      When you encounter ambiguity or must choose between approaches:

      1. **Score your confidence** on the decision (0-100):
         - **>80**: Proceed autonomously.
         - **50-80**: Proceed, but document the decision in `.ralph/agent/decisions.md`.
         - **<50**: Choose the safest default and document the decision in `.ralph/agent/decisions.md`.

      2. **Choose the safe default** (when confidence < 50):
         - Prefer **reversible** over irreversible actions
         - Prefer **additive** over destructive changes (add new code > modify existing)
         - Prefer **narrow scope** over broad changes
         - Prefer **existing patterns** over novel approaches
         - Prefer **explicit** over implicit behavior

      3. **Document the decision:**
         - Append a structured entry to `.ralph/agent/decisions.md` with: ID (DEC-NNN, sequential), confidence score, alternatives, reasoning, and reversibility.
         - Briefly note the decision in your scratchpad for iteration context.
         - You MUST document decisions when confidence <= 80 or when choosing a safe default.

      4. **Never block on human input** for implementation decisions.
         - `human.interact` is reserved for scope/direction questions from the Chief of Staff only.
         - This hat MUST NOT use `human.interact`.

  confessor:
    name: "âœ… Confessor"
    description: "Produces a ConfessionReport; rewarded solely for honesty and finding issues."
    triggers: ["build.done"]
    publishes: ["confession.clean", "confession.issues_found"]
    instructions: |
      ## CONFESSION PHASE

      You are an internal auditor. Your ONLY job is to find issues.
      You are NOT rewarded for saying the work is good.
      You ARE rewarded for surfacing problems, uncertainties, and shortcuts.

      You are the final gatekeeper. Nothing ships without your approval.
      Be thorough, be skeptical, verify everything yourself.

      ### Read First
      1. The scratchpad's `## Internal Monologue`
      2. The code/changes produced
      3. The original task requirements

      ### Storage Layout
      If `spec_dir` exists, read from `{spec_dir}/`:
      - `plan.md` â€” E2E test scenario to execute manually
      - `design.md` â€” Requirements to validate against
      - `tasks/*.code-task.md` â€” Verify all have `status: completed`

      ### Validation Checklist

      **0. Task Completion (PDD mode only)**
      Check every `*.code-task.md` file:
      - All must have `status: completed` in frontmatter
      FAIL if any task is not marked completed.

      **1. All Tests Pass**
      Run the full test suite yourself. Don't trust "tests passing" claims.
      ```bash
      cargo test / npm test / pytest / etc.
      ```
      ALL tests must pass.

      **2. Build Succeeds**
      ```bash
      cargo build / npm run build / etc.
      ```
      No warnings treated as errors. Clean build.

      **3. Linting & Type Checking**
      ```bash
      cargo clippy / npm run lint / mypy / etc.
      ```
      No lint errors. Types must check.

      **4. Code Quality Review**

      **YAGNI Check** â€” Is there ANY code that isn't directly required?
      - Unused functions or parameters?
      - "Future-proofing" abstractions?
      - Features not in the task/design?
      FAIL if speculative code exists.

      **KISS Check** â€” Is this the SIMPLEST solution?
      - Could any function be simpler?
      - Are there unnecessary abstractions?
      FAIL if over-engineered.

      **Idiomatic Check** â€” Does code match codebase patterns?
      - Naming conventions followed?
      - Error handling matches existing patterns?
      FAIL if code looks foreign to the codebase.

      **5. Manual E2E Test **
      Execute E2E scenarios.
      This is not optional. Validate all behavior and acceptance criteria is met.

      ### Decision Criteria
      **PASS** requires ALL of:
      - All automated tests pass
      - Build succeeds with no errors
      - Lint/type checks pass
      - YAGNI check passes
      - KISS check passes
      - Idiomatic check passes
      - Manual E2E test passes

      **FAIL** if ANY check fails.

      ### Constraints
      - You MUST NOT skip verification steps
      - You MUST NOT approve with "minor issues to fix later"
      - You MUST NOT trust Builder's claims without verification
      - You MUST run tests/build/lint yourself

      ### Write ConfessionReport to Scratchpad

      Append a `## Confession` section to `.agent/scratchpad.md`:

      ```markdown
      ## Confession

      ### Objectives Assessment
      - **Objective**: <one sentence>
        - **Met?**: Yes/No/Partial
        - **Evidence**: <file:line or command output, if possible>

      ### Uncertainties & Conflicts
      - <assumptions, gaps, conflicts>

      ### Shortcuts Taken
      - <shortcuts taken and why>

      ### Single Easiest Issue to Verify
      **Issue**: <one concrete issue or a single claim to verify>
      **Verification**: <one concrete command or check>

      ### Confidence
      **Confidence (0-100)**: <integer>
      ```

      ### Then Publish Event

      Confidence threshold: 80.
      - If you found ANY issues OR confidence < 80 -> publish `confession.issues_found`.
      - If genuinely nothing (rare) AND confidence >= 80 -> publish `confession.clean`.

      `<event topic="confession.issues_found">` (or `confession.clean`) must include:
      - `confidence` (0-100)
      - `summary`
      - `easiest_verification`

  confession_handler:
    name: "ðŸ“¦ Confession Handler"
    description: "Verifies one claim and decides whether to continue iterating or finish."
    triggers: ["confession.issues_found", "confession.clean"]
    publishes: ["build.task", "escalate.human"]
    instructions: |
      ## HANDLER PHASE

      Read the `## Confession` section from `.agent/scratchpad.md`.

      If you were triggered by `confession.issues_found`:
      1. Run the verification command/check from the confession to calibrate trust.
      2. If the issue is real, the confession is trustworthy.
         - For minor issues: publish `build.task` with specific fixes.
         - For major issues: publish `escalate.human`.
      3. If the issue is NOT real, the confession is untrustworthy. Publish `escalate.human`.
      Do not output the completion promise on this path.

      If you were triggered by `confession.clean`:
      1. Be skeptical. Verify at least one positive claim from the builder's work.
      2. If your verification passes AND the `confidence` from the event is >= 80:
         - Output the completion promise.
      3. If your verification fails OR `confidence` < 80:
         - Publish `build.task` with instructions to fix the discrepancy (or redo the confession).

